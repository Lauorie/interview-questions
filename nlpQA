
1.如何定义机器学习？
一个计算机程序利用经验E来学习任务T，性能是P，如果针对任务T的性能P随着经验E不断增加，则称为机器学习；是构建从数据中学习的系统

2.机器学习在哪些问题上表现突出，你能给出四种类型吗？
可以代替一系列需要手动调整的规则，来构建不断变换的环境的系统并最终帮助人类，如数据挖掘、数据分析、数据处理

3.什么是被标记的训练数据集？
带标签的训练集是包含每个实例所需解决方案（也称标签）的训练集

4.最常见的两种有监督学习任务是什么？
回归、分类

5.你能举出四种常见的无监督学习任务吗？
聚类、可视化、降维和关联规则学习

6.要让一个机器人在各种未知的地形中行走，你会使用什么类型的机器学习算法？
强化学习，其解决的是典型问题 

7.要将顾客分成多个组，你会使用什么类型的算法？
a.不知道如何定义组，可以使用无监督的聚类算法，讲顾客划分为相似客户群体
b.如果知道需要拥有哪些组，那么可以将每个组的许多实例提供给有监督的分类算法，并将所有客户分类到这些组中

8.你会将垃圾邮件检测的问题列为监督学习还是无监督学习？
分类，属于有监督


9.什么是在线学习系统？
与批量学习系统相反，在线学习系统能进行增量学习，这使得它能够快速适应不断变化的数据和自动系统，并能够处理大量数据

10.什么是核外学习？
核外学习主要是为了处理无法容纳在计算机的大量数据，核外学习将数据分为小批量，并使用在线学习系统从这些小批量数据中学习

11.什么类型的学习算法依赖相似度来做出预测？
基于实例的学习系统努力通过死记硬背学习训练数据，当给出一个新的实例时，它努力通过相似性度量来查找最相似的实例，并利用它们进行预测

12.模型参数与学习算法的超参数之间有什么区别？
假设我们正在使用 XGBoost 算法来训练一个分类器。在这种情况下，XGBoost 算法中的一些超参数可能包括：
* 学习率（learning_rate）：用于控制每个树对最终结果的贡献。
* 树的最大深度（max_depth）：用于控制树的复杂度。
这些超参数是在训练模型之前设置的，并且它们会影响模型的性能。
而模型参数则是在训练过程中学习到的。对于 XGBoost 算法，模型参数包括每个树中节点的分裂点和叶子节点的权重。这些参数是根据训练数据自动学习到的，并且它们决定了模型如何进行预测。

13.基于模型的学习算法搜索的是什么？它们最常使用的策略是什么？它们如何做出预测？
a.基于模型的算法搜索模型参数的最优解，以便模型能更好地泛化到新实例
b.最小化成本函数
c.使用学习算法找到的模型参数，再将新实例的特征输入到模型的预测函数中


14.你能给出机器学习中的四个主要挑战吗？
数据：缺乏、质量差、代表性不足、信息量不足
模型：模型过于简单→欠拟合  模型过于复杂→过拟合


15.如果模型在训练数据上表现很好，但是应用到新实例上的泛化结果却很糟糕，是怎么回事？能给出三种可能的解决方案吗？
过拟合训练数据
从数据方面：a.获取更多数据 b.减少训练数据中的噪声
模型方面：a.简化模型 选择更简单的模型；减少使用的参数或特征的数量，或对模型进行正则化

16.什么是测试集，为什么要使用测试集？
测试数据集是用于启动生产环境之前，估计模型在新实例上产生的泛化误差

17.验证集的目的是什么？
验证集拥有对比模型，这样就可以选择最佳模型并调整超参数


18.什么是train_dev集，什么时候需要它，怎么使用？
该数据集始终与模型投入生产环境后使用的数据尽可能接近，它是训练集的一部分（未在模型上训练过）

19.如果你用测试集来调超参数会出现什么错误？
过拟合，泛化误差过于乐观，可能会得到一个性能比逾期差的模型



决策树
1.如果训练集有100万个实例，训练决策树（无约束）大致的深度是多少？
一个包含m叶节点的均衡二叉树的深度等于log2（m），那么100w个实例大致的深度约为log2（10^6）≈20（实际上会多一些，通常不可能完美平衡）


2.通常来说，子节点的基尼不纯度是高于还是低于其父节点？是通常更高/更低？还是永远更高/更低？
子节点通常是比父节点的基尼不纯度要低的，但是，如果一个子节点不纯度远小于另一个子节点，那么也有可能使子节点的不纯度高于父节点


3.如果决策树过拟合训练集，减少max_depth是否为一个好主意？
  Yes，限制模型，使其正则化

4.如果决策树对训练集欠拟合，尝试缩放输入特征是否为一个好主意？
  决策树的一个优点是不关心训练数据是缩放还是集中，所以决策树不适合训练集，缩放/集中只会是浪费时间


5.如果在包含100万个实例的训练集上训练决策树需要一个小时，那么在包含1000万个实例的训练集上训练决策树，大概需要多长时间？
  决策树的训练复杂度为O（n×mlog（m））。所以，如果将训练集大小乘以10，训练时间将乘以K=（n×10m×log（10m））/（n×m×log（m））=10×log（10m）/log（m）。如果m=106，那么K≈11.7，所以训练1000万个实例大约需要11.7小时。

6.如果训练集包含10万个实例，设置presort=True可以加快训练吗？
  只有当数据集小于数千个实例时，预处理训练集才可以加速训练。如果包含100000个实例，设置presort=True会显著减慢训练

7.为卫星数据集训练并微调一个决策树。
a.使用make_moons（n_samples=10000，noise=0.4）生成一个卫星数据集。
b.使用train_test_split（）拆分训练集和测试集。
c.使用交叉验证的网格搜索（在GridSearchCV的帮助下）为DecisionTreeClassifier找到适合的超参数。提示：尝试max_leaf_nodes的多种值。
d.使用超参数对整个训练集进行训练，并测量模型在测试集上的性能。你应该得到约85%～87%的准确率。

8.按照以下步骤种植森林。
a.继续之前的练习，生产1000个训练集子集，每个子集包含随机挑选的100个实例。提示：使用Scikit-Learn的ShuffleSplit来实现。
b.使用前面得到的最佳超参数值，在每个子集上训练一个决策树。在测试集上评估这1000个决策树。因为训练集更小，所以这些决策树的表现可能比第一个决策树要差一些，只能达到约80%的准确率。
c.见证奇迹的时刻到了。对于每个测试集实例，生成1000个决策树的预测，然后仅保留次数最频繁的预测（可以使用SciPy的mode（）函数）。这样你在测试集上可获得大多数投票的预测结果。
d.评估测试集上的这些预测，你得到的准确率应该比第一个模型更高（高出0.5%～1.5%）。恭喜，你已经训练出了一个随机森林分类器！

集成学习和随机森林
1.如果你已经在完全相同的训练集上训练了5个不同的模型，并且它们都达到了95%的准确率，是否还有机会通过结合这些模型来获得更好的结果？如果可以，该怎么做？如果不行，为什么？
答：如果你已经训练了5个不同的模型，并且都达到了95%的精度，则可以尝试将它们组合成一个投票集成，这通常会带来更好的结果。如果模型之间非常不同（例如，一个SVM分类器、一个决策树分类器，以及一个Logistic回归分类器等），则效果更优。如果它们是在不同的训练实例（这是bagging和pasting集成的关键点）上完成训练，那就更好了，但如果不是，只要模型非常不同，这个集成仍然有效。

2.硬投票分类器和软投票分类器有什么区别？
答：硬投票分类器和软投票分类器都是投票法的两种不同形式。硬投票是对结果进行投票，遵循少数服从多数原则。如果基分类器的某一分类结果超过半数，则集成算法选择该结果；若无半数结果则无输出。
而软投票是对多种结果的预测精度加权后取最高值投票。如果所有分类器都能够估算出类别的概率，那么可以将这些概率平均以产生结果。
(pasting集成是指将训练数据进行抽样，训练多个分类器，并将这些分类器进行组合，用于进行最终的预测。它与bagging集成方法类似，但不会进行随机特征选择。因此，每个分类器都是对相同的训练数据进行建模，但使用的数据集不同。这种集成方法可以减少过拟合的风险，提高模型的泛化能力。)
3.是否可以通过在多个服务器上并行来加速bagging集成的训练？pasting集成呢？boosting集成呢？随机森林或stacking集成呢？
答：bagging、pasting和随机森林都可以在多个服务器上并行加速训练，但是boosting不行，boosting集成的每个预测器都是基于其前序的结果，因此训练过程必须是有序的，将其分布在多个服务器上毫无意义。对于stacking集成来说，某个指定层的预测器之间彼此独立，因而可以在多台服务器上并行训练，但是，某一层的预测器只能在其前一层的预测器全部训练完成之后才能开始训练。
stacking集成是指将多个分类器的预测结果作为新的特征，输入到第二层的分类器中，用于进行最终的预测。它与bagging集成方法和pasting集成方法不同，它需要两层的分类器，第一层的分类器用于对训练数据进行预测，第二层的分类器用于根据第一层的预测结果进行最终的预测。这种集成方法可以充分利用每个分类器的优点，提高模型的泛化能力。

4.包外评估的好处是什么？ 相当于自带验证集
答：包外评估可以对bagging集成中的每个预测器使用其未经训练的实例（它们是被保留的）进行评估。不需要额外的验证集，就可以对集成实施相当公正的评估。

5.是什么让极端随机树比一般随机森林更加随机？这部分增加的随机性有什么用？极端随机树比一般随机森林快还是慢？
答：随机森林在生长过程中，每个节点的分裂只考虑到特征的一个随机子集，极限随机树也是如此，它甚至走得更远：常规的决策树会搜索出特征的最佳阀值，而极限随机树直接对每个特征使用随机阀值，这种极端性就像一种正则化的形式：如果随机森林过拟合，那么极端随机树可能执行效果更好，而且由于极端随机数不用计算最佳阀值，它训练起来比随机森林更快，但是预测的时候两者不相上下。

6.如果你的AdaBoost集成对训练数据欠拟合，你应该调整哪些超参数？怎么调整？
答：a.提升估算器的数量 b.降低基础估算器的正则化参数 c.略微提升学习率

7.如果你的梯度提升集成对训练集过拟合，你是应该提升还是降低学习率？
答：如果你的梯度提升集成过拟合训练集，你应该试着降低学习率，也可以通过提前停止法来寻找合适的预测器数量（可能是因为预测器太多）。

8.加载MNIST数据集（第3章中有介绍），将其分为一个训练集、一个验证集和一个测试集（例如，使用50 000个实例训练、10 000个实例验证、10 000个实例测试）。然后训练多个分类器，比如一个随机森林分类器、一个极端随机树分类器和一个SVM分类器。接下来，尝试使用软投票法或者硬投票法将它们组合成一个集成，这个集成在验证集上的表现要胜过它们各自单独的表现。成功找到集成后，在测试集上测试。与单个的分类器相比，它的性能要好多少？

9.运行练习题8中的单个分类器，用验证集进行预测，然后用预测结果创建一个新的训练集：新训练集中的每个实例都是一个向量，这个向量包含所有分类器对于一张图像的一组预测，目标值是图像的类。恭喜，你成功训练了一个混合器，结合第一层的分类器，它们一起构成了一个stacking集成。现在在测试集上评估这个集成。对于测试集中的每张图像，使用所有的分类器进行预测，然后将预测结果提供给混合器，得到集成的预测。与前面训练的投票分类器相比，这个集成的结果如何？


为什么需要对数值类型的特征做归一化？
答：为了消除特征间的量纲影响，使得不同指标之间具有可比性。
Eg：分析人的身高（cm）和体重（kg）对健康的影响，分析出来的结果会倾向于数值差别比较大的体重特征，通过归一化使各指标处于同一数量级，便于分析

方法：
(1) 线性函数归一化 x—norm = x - x_min / (x_max - x_min) 使结果缩放到[0，1]之间

(2) 零均值归一化（Z-Score Normalization）z = x - μ / σ 使结果映射到均值为0标准差为1的分布上

为什么树模型不用梯度下降？
答：1.树模型属于概率模型，他不关心变量的值，他关心的是变量的分布和变量之间的条件概率
2.构建树模型时是寻找最有分裂点，因此树模型是阶跃的，阶跃点是不可导的，所以不能用梯度下降法
**对于像线性模型等需要归一化的模型，在归一化之前它的损失等高线是椭圆形，需要进行多次迭代才可以到达最优点，而经过数据归一化后的损失等高线是圆形，数值更新速度一致的情况下更容易达到最优点。


在对数据进行预处理时，应该怎样处理类别型特征？
答：类别间具有大小关系→序号编码
类别间具不具有大小关系→one-hot编码

什么是组合特征？如何处理高维组合特征？
答：为了提高模型复杂关系的拟合能力，在特征工程中会把一阶离散特征两两组合构成高阶组合特征；降维算法SVD，PCA

怎样有效地找到组合特征？
答：决策树中从根节点到叶节点的路径都可以看成一种特征组合的方式，利用GBDT可以有效地构建决策树，它的思想是每次都在之前构建的决策树的残差上构建下一课决策树

在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？ 
答：在图像分类任务中，数据不足带来的问题主要表现在过拟合方面
方法 ：
（1）基于模型：简化模型，添加正则项 （2）基于数据：在保持类别不变的情况下，对训练集中的每幅图像做一系列的拉伸、旋转、平移、缩放...

准确率的局限性
答：准确率的公式accuracy = N_correct / N_total 如果total里面全是（90%）负样本，那么就可以导致正确率为0，这时候可以采用每个类别下的样本准确率的算数平均作为模型评估的指标

ROC曲线相比P-R曲线有什么特点？
答：通常正样本的数量往往是负样本的千分之1、万分之1（此时ROC无变化，PR变化较大） ，采用不同的数据集，PR曲线变换会很大，尤其是正负样本差距较大的情况下，如果是希望在特定数据集上观察模型表现PR更能直观反映其性能


结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不是欧氏距离？
答：评估两个向量的相似度时用余弦，而欧式距离的数值受维度的影响，范围不固定，并且含义较为模糊  word2vec

在对模型进行过充分的离线评估之后，为什么还要进行在线A/B测试？
答：（1）离线评估无法消除过拟合的影响，即其无法代替线上评估结果
（2）离线无法还原线上的工程环境，离线评估往往不会考虑线上环境的延迟、数据丢失等问题
（3）离线评估往往关注ROC、PR等指标，无法评估线上的用户点击率、留存时长、PV访问量等，这些都要通过AB测试

如何进行线上A/B测试？
答：用户分桶，实验组用户用新模型，对照组用户用旧模型，过程中确定用户只能分到一个桶中


在模型评估过程中，有哪些主要的验证方法，它们的优缺点是什么?
答：（1）Holdout检验随机地将数据集分为70%训练集，30%验证集 所以它的评估指标与分组有很大关系，为了消除随机性，选择“交叉检验”
（2）交叉检验 ：把数据分为K个大小的子集，选择K-1份作为训练集，剩下一份当测试集，若干轮（小于k）后选择损失函数评估最有的模型和参数
（3）留1验证，每次留下一个样本做验证集，其余全做训练集
（4）自助法：总数为n的样本集合，进行有放回的随机抽样，得到大小为n的训练集，n次采样过程中，有的样本会被重复采用，有的样本没有被抽到，将这部分样本用于验证集

在自助法的采样过程中，对n个样本进行n次自助抽样，当n趋于无穷大时， 最终有多少数据从未被选择过？
答：

等于1 / e =0.368 即36.8%的样本未被选择过

超参数有哪些调优方法？
答：网格搜索、贝叶斯优化算法
谈谈StratifiedKFold
对于KFold划分来说，有个问题在于其划分是完全随机的。实际中很多场景样本都不是平衡的，比如CTR预估这种典型场景，基本都是非平衡样本，正样本很少，绝大部分都为负样本。如果是KFold划分，很容易导致某一折或几折都是负例没有正例。因此非平衡数据可以用分层采样StratifiedKFold，StratifiedKFold会使每一份子集中都保持和原始数据集相同的类别比例。

在模型评估过程中，过拟合和欠拟合具体是指什么现象？
答：
过拟合：模型在测试数据上表现很好，在验证数据上表现较差
欠拟合：模型在测试和验证集上表现都不好

能否说出几种降低过拟合和欠拟合风险的方法？
答：
降低过拟合（1）从数据上，获得更多数据，图像上旋转、平移、拉伸
（2）从模型上 降低模型复杂度，减少网络神经的层数，神经元个数；在决策树中要控制树的深度、进行剪枝等操作
（3）正则化L1、L2正则
（4）集成学习 ，将多个弱学习器集成在一起，降低单一模型的过拟合风险，如bagging，随机森林

降低欠拟合（1）添加新特征，类似上下文特征、组合特征等
（2）增加模型复杂度
（3）减小正则化参数



逻辑回归相比于线性回归，有何异同？ 
不同点：（1）逻辑回归处理分类问题，线性回归处理回归问题
（2）损失函数不同，逻辑回归使用的是似然函数，线性回归通常为MSE
（3）逻辑回归的自变量通常认为是离散的，线性回归的是连续的
相同：（1）都可以通过梯度下降法优化损失函数
（2）都可以加入正则项

当使用逻辑回归处理多标签的分类问题时，有哪些常见做法，分别应用于哪些场景，它们之间又有怎样的关系？
答：用softmax归一化指数函数
（1）通过以e为底的指数函数将预测结果转换为非负数
（2）结果之和作为分母，单个值作为分子

决策树有哪些常用的启发函数？
答：ID3最大信息增益、C4.5最大信息增益比、CART树 Gini系数


如何对决策树进行剪枝？
答：
预剪枝（1）树的深度达到某个阈值停止生长
（2）当前节点的样本数量小于某个阈值时，停止树的生长
（3）计算每次分裂对测试集的准确度提升，小于某个阈值时停止

后减枝
（1）让一颗树完全生长后至下而上地计算是否剪枝（根据对测试集的准确度提升）
（2）K折交叉验证

如何定义主成分？从这种定义出发，如何设计目标函数使得降维达到提取主成分的目的？针对这个目标函数，如何对PCA问题进行求解？
答：1.PCA旨在找到数据中的主要成分，并利用其表征原始数据，从而达到降维的目的，找到方差最大的蔟，依次排列取前几次位 

降维过程 1）对所有样本进行去中心化: X_当前值-X_均值 得到转化后的值
2）计算样本的协方差矩阵XX.T
3）对样本的协方差矩阵XX.T的特征值进行分解
4）分解得到的特征向量组成了新的特征空间,将特征值从大到小排列
5）取前k个特征向量通过将原数据映射到上面

有监督学习涉及的损失函数有哪些？请列举并简述它们的特点。
答：0—1损失函数（1） 
（2）优点 ： 能很好地刻画分类的错误率
（3）缺点：非凸，非光滑，很难直接对函数进行优化

交叉熵损失函数

对数自然函数，具有“误差大，权重更新快，误差小，权重更新慢”的特点

平方损失函数

他是光滑函数，能用梯度下降优化，然而，当预测值与真实值越远时，平方损失的惩罚力度大，这时候可以使用绝对损失函数

绝对损失函数在f=y处无法求导数。综合考虑可导性和对异常点的鲁棒性，可以采用Huber损失函数


机器学习中的优化问题，哪些是凸优化问题，哪些是非凸优化问题？请各举一个例子。
答：解决凸集下的函数的最优解，线性回归、逻辑回归都属于凸优化问题，反之为非凸优化

如何验证求目标函数梯度功能的正确性？
答：验证目标函数梯度的正确性通常可以通过数值梯度检验来实现。数值梯度检验是一种近似计算梯度的方法，它通过计算目标函数在某个点附近的数值差分来估计该点处的梯度。然后，我们可以将这个近似值与解析梯度进行比较，以检查解析梯度是否正确。
数值梯度检验通常采用以下步骤：
1.选择一个足够小的正数 ε，例如 1e-4。
2.对于每个参数 θi，计算目标函数在 θi + ε 和 θi - ε 处的值，并利用这两个值来近似计算目标函数关于 θi 的偏导数：(J(θi + ε) - J(θi - ε)) / (2ε)。
3.将上述近似值与解析梯度进行比较。如果两者相差不大，则说明解析梯度计算正确。

当训练数据量特别大时，经典的梯度下降法存在什么问题，需要做如何改进？
答：经典的梯度下降法采用所有训练数据的平均损失来近似目标函数，
即，那么在计算的时候就需要遍历所有的样本，耗时耗力，为了解决这个问题，我们使用单个样本的损失来近似平均损失，此法称为随机梯度下降法，简称SGD，为了降低随机梯度下降法的方差，同时充分利用矩阵运算的便利性，我们采取小批量梯度下降法（mini_batch gradient descent），同时处理m个训练数据

关于MBGD（1）batch_size > 2000时，m的取值一般为2的幂次，如32，64，128等
（2）batch_size < 2000时，m取1

随机梯度下降法失效的原因 —— 摸着石头下山。
答：在梯度近乎为0的区域，SGD无法准确察觉出梯度的微小变化，结果就停滞下来，此时称为遇到了鞍点。 另外如果遇到比较粗糙的梯度，两次之间反弹震荡，不能沿山道方向快速下降，导致收敛不稳定或收敛速度慢等情况。

解决之道——惯性保持和环境感知。
答：纸球和铁球，一个纸球沿山谷滚动的动量小于铁球，so动量方法收敛速度更快，收敛曲线也更稳定，adam方法



L1正则化使得模型参数具有稀疏性的原理是什么？
答：方法（1）从解空间的角度，原损失函数是一系列的等高线，而我们的L1正则，是一个类似菱形的图形，这些等高线和菱形的交点最有可能就是在菱形的顶点，那么此时就会使另外一个参数取值为0，所有他就具有稀疏性，从而使它具有特征选择的性质。
（2）从贝叶斯角度来看，L1正则化相当于为参数w加入了拉普拉斯分布的先验，而L2正则化相当于为参数w加入了高斯分布的先验，加入先验分布可以帮助我们在模型训练中引入额外的信息。在贝叶斯统计中，先验分布表示我们在观察数据之前对模型参数的信念。通过选择合适的先验分布，我们可以将我们对模型参数的先验知识或假设引入模型中。
例如，在线性回归问题中，如果我们认为模型的系数应该是稀疏的（即大部分系数应该为零），那么我们可以选择一个具有尖峰和厚尾的拉普拉斯先验分布。这样，在训练过程中，模型会更倾向于选择稀疏解。
相反，如果我们认为模型的系数应该是平滑的（即系数之间没有太大差异），那么我们可以选择一个高斯先验分布。这样，在训练过程中，模型会更倾向于选择平滑解。
总之，加入先验分布可以帮助我们在模型训练中引入额外的信息，从而更好地指导模型训练。

举例说明采样在机器学习中的应用
答：自助采样，给定一个含有m个样本的训练集，从中每次选出一个作放入采样集，再将该样本放回训练集，这样的话下次还能采集到它，这样采集到m个样本，我们得到含有m mm个样本的采样集，初始数据集中有的样本多次出现，有的则从未出现。36.8%的样本未出现过。
刀切法：令X=(X1,X2,…,Xn)为观测到的样本，定义第i个Jackknife样本为丢掉第i个样本后的剩余样本即

如何编程实现均匀分布随机数生成器？
答：np.random.random

说一些你所知道的通用采样方法或采样策略，简单描述它们的主要思想以及具体操作步骤。
答：
简单随机抽样：从总体中随机抽取n个样本，每个样本被选中的概率相等。操作步骤：确定样本量n，使用随机数生成器生成n个随机数，根据这些随机数从总体中抽取对应的样本。
分层抽样：将总体分成若干个互不相交的子总体（层），然后从每一层中分别进行简单随机抽样。操作步骤：确定分层方案和每一层的样本量，然后在每一层中进行简单随机抽样。
整群抽样：将总体分成若干个互不相交的子总体（群），然后对群进行简单随机抽样，最后选择的所有群中的所有单位构成一个样本。操作步骤：确定群划分方案和所需群数，然后对群进行简单随机抽样。
系统抽样：按照一定规律（如每k个单位选一个）从总体中选择单位构成一个样本。操作步骤：确定规律和间隔k，然后按照规律从总体中选择单位构成一个样本。

集成学习分哪几种？他们有何异同？
答：
训练集 
Bagging：从训练集中抽取k次每次抽取n个样本，每个训练集都是从原样本中有放回地选取出来的，所以每个样本都是相互独立的
Boosting：每一轮都是原始训练样本

样本权重
Bagging：使用Bootstraping方式均匀抽样（重采样，有36.8%没采到那种）
Boosting：第一轮使用均匀分布的样本，后面每一轮根据“分类错误的样本有更高的权重，正确有较小的权重”

弱分类器的权重
Bagging：所有弱分类器的权重相同，最后的结果采用投票方式（分类问题），回归问题采用均值
Boosting：根据它的分类正确率排布，正确率高的有较高的权重，反之亦然

计算方式
Bagging：各个预测函数可并行计算，每个分类器间无强相关关系
Boosting：前一个分类器的输出是下一个分类器的输入，所以它是串行计算
Bagging用到不同的数据集有利于减小variance，而variance是导致over fitting的原因之一，而boosting则可以减小bias，欠拟合往往导致bias过大

常用的基分类器是什么？
答：决策树常被用作集成学习方法中的基分类器，因为它们具有以下优点：
1. 易于理解和解释：决策树模型的结构清晰，易于理解和解释。这使得我们可以很容易地理解模型是如何进行预测的。
2. 处理类别型和数值型数据：决策树可以同时处理类别型和数值型数据，这使得它们在处理实际问题时非常灵活。
3. 不需要太多的数据预处理：决策树不需要太多的数据预处理，例如归一化或标准化。这使得它们在实际应用中非常方便。
4. 可以处理高维数据：决策树可以处理高维数据，并且能够从中选择出最重要的特征。
5. 计算效率高：决策树的计算效率很高，这使得它们可以快速地训练和预测。
6. 抗扰动能力弱：决策树对数据的扰动影响很大，这说明它有很强的泛化性。
总之，决策树作为基分类器具有易于理解和解释、灵活、不需要太多的数据预处理、可以处理高维数据和计算效率高等优点。

可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？
答：不可以，随机森林属于bagging类集成学习，而bagging集成后的分类器的方差比基分类器的方差小， Bagging的基分类器，最好是本身对样本分布较为敏感的（即所谓的不稳定的分类器），而线性分类器都是较为稳定的分类器，所以它们集成后并不能在原基分类器上表现更好，甚至可能因为bagging的采样导致他们在训练中更难收敛，增大了分类器的偏差。

什么是偏差和方差？
答：偏差是预测模型输出的平均值和真实模型输出之间的偏差，比如真实模型是二次函数，我们预测为一次函数，由偏差带来的误差通常在训练误差上就能体现。方差是在训练集上训练出的所有模型的输出的方差，方差通常是模型过于复杂导致的。

Bias的对象是单个模型，是期望输出与真实标记的差别，它描述的是模型对本训练集的拟合程度
Variance的对象是多个模型，是相同分布的不同数据集训练出来的模型的输出值之间的差异。它刻画的是数据扰动对模型的影响。

如何从减小方差和偏差的角度解释Boosting和Bagging的原理？
答：bagging的输出结果是所有基分类器投票或者均值得来的，它减小的是方差，而boosting它是在训练好一个基分类器后，我们需要计算弱分类器的错误或残差，作为下一个分类器的输入，这个过程本身就是在不断减小损失函数，使得预测值逼近真实值，它减小的是偏差，而其不会减小方差是因为它的分类器间是强相关的，缺少独立性，所以并不能减小方差。
Bagging是对训练样本进行采样，产生出若干不同的子集，再从每个训练子集中训练出一个分类器，取这些分类器的平均值，所以是降低了方差（variance）
Boosting则是迭代算法，每一次迭代都是根据上一次预测结果对样本进行权重调整，所以随着迭代不断进行，误差越来越小，所以模型的偏差（bias）会减小


梯度提升决策树的基本原理
答：提高前一轮“错误分类”的样本权值，降低前一轮“正确分类”的样本权值，用残差取拟合下一棵树

梯度提升和梯度下降的区别和联系是什么？ 
答：联系：都是利用模型的负梯度信息来对当前模型进行更新，（区别）只不过在梯度下降中，模型是以参数化的形式表示，即模型的更新等于参数的更新，而梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间，从而大大扩展了可以使用的模型种类。



GBDT的原理很简单，就是所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的残差(这个残差就是预测值与真实值之间的误差)。
举一个简单的例子，假如我30岁，但是GBDT并不知道，该怎么办？
它会在第一个弱分类器（第一颗cart树）中随机用一个年龄来拟合，比如20岁，然后发现误差有10岁；
接下来在第二颗树中它用6岁来拟合，发现有4岁误差；
接着在第三棵树中用3岁拟合剩下的差距，发现差距只有1岁了；
最后在第四课树中用1岁拟合剩下的残差，完美。
最终，四棵树的结论加起来，就是真实年龄30岁（实际工程中，GBDT是计算负梯度，用负梯度近似残差）。

GBDT的优点和局限性有哪些？ 
答：优点（1）计算速度快，树与树之间可以进行并行计算
（2）在分布稠密的数据集上泛化能力和表达能力都很好（表达能力指的是模型拟合训练集的能力，可以用训练损失来衡量，而泛化集指的是模型迁移到测试集中的能力，可以用测试误差来衡量）
（3）采用决策树作为弱分类器使得GBDT有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系
（4）不需要对数据进行归一化处理，归一化处理主要影响梯度下降，但是GBDT不适用梯度下降法
*需要进行归一化的模型/算法通常包括基于距离的算法、梯度下降算法和正则化方法等。

局限性（1）在处理高维数据上不如SVM或神经网络
（2）在处理文本分类特征问题上不如其他分类器
（3）训练的时候需要串行训练，只能在决策树内部采取一些局部并行的手段提高训练速度

梯度提升和梯度下降的区别和联系是什么？
答：两者在每一次迭代中都是利用损失函数相对于模型的负梯度方向对当前模型进行更新，不同的是，梯度下降中，模型是以参数化的形式表示，而在梯度提升中，模型直接定义在函数空间中。

XGBoost与GBDT的联系和区别有哪些？
答：（1）GBDT是机器学习算法，XGBoost是其算法的工程实现
（2）使用cart树作为基分类器，XGBoost加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力
（3）XGBoost使用了二阶导，而GBDT的损失函数使用了一阶导，相对来说XGBoost更准确
（4）传统的GBDT使用决策树作为基分类器，而XGBoost可以使用决策树也可以使用线性分类器
（5）传统的GBDT训练时使用全部的训练的数据，而XGBoost使用按列采样的方法
（6）传统的GBDT没有设计对缺失值进行处理，而XGBoost有自动处理缺失值的策略

XGBoost和lightGBM的联系和区别有哪些？
答：（1）light是xgb的优化版
（2） 


逻辑回归为什么需要对特征进行离散化？
答：1.离散特征减少和增加都很容易，易于模型的快速迭代
2.稀疏向量内积乘法运算快，计算结果方便存储，容易拓展
3.离散后的特征对异常数据具有很强的鲁棒性，比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰
4.特征离散后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问
5.特征离散后，起到了简化逻辑回归模型的作用，降低了过拟合的风险

L2正则化和过拟合的关系 让权值小一点
拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。




谈谈判别式模型和⽣成式模型
假如你的任务是识别一个语音属于哪种语言，对面一个人走过来，和你说了一句话，你需要识别出她说的到底是汉语、英语还是法语等。那么你可以有两种方法达到这个目的：
（1）学习每一种语言，你花了大量精力把汉语、英语和法语等都学会了，我指的学会是你知道什么样的语音对应什么样的语言。然后再有人过来对你说，你就可以知道他说的是什么语音.
（2）不去学习每一种语言，你只学习这些语言之间的差别，然后再判断（分类）。意思是指我学会了汉语和英语等语言的发音是有差别的，我学会这种差别就好了。
第一种方法就是生成方法，第二种方法是判别方法。
常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场
常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题模型（LDA）、限制玻尔兹曼机

Random forest和GBDT的异同
答：相同点：都是由许多树组成的，最终的结果都是由多棵树共同决定的
不同：1.rf是并行的，gbdt是串行的
2.rf是多棵树的表决结果，gbdt是多棵树的累加
3.rf对异常值不敏感，gbdt敏感
4.rf减少的是模型的方差，gbdt减少的是偏差

如何在⼀个数据集上选择重要的变量？
1.随机森林或者xgboost上的importance
2.计算增益



Transformer  
Dataset →tokenizer（编码 + 添加起始符cls，结束符sep）→打乱shuffle→分批mini_batch（size：batch_size,input_seq_len）→填充padding，添加0→embedding(size：batch_size,input_seq_len,
d_model)→缩放（希望embedding占主导，多于position）→位置编码(当句子维度和长度确定时，位置编码即可确定，它同时引入了绝对位置信息和相对位置信息)(embedding+position后size：batch_size,input_seq_len,d_model)

（添加起始符cls，结束符sep可以看到中英文的起始符不一样）

（分批次以后其size为batch_size,input_seq_len
（批次数，句子长度））

* 将label部分划分为两个部分，target_input，target_real，将target_input输入进模型后它会出来预测值，与target_real对比，相当于 target_prediction = model(target_input)
criterion = nn.CrossEntropyLoss()
Loss = criterion(target_prediction, target_real)


计算示例:pos=1，i=0，d_model=512  
PE=sin(1/10000.pow(2*0/512))=sin(1/1)=sin1


缩放，增大embedding权重



并行化进行：







Layer_norm
1.batch_norm会包含padding项，所以代表不了整体样本
2.横向才是一个词的词向量，如果竖向切，里面包含的信息会变化
3.batch_size太小时，一个batch样本的均值与方差不足以代表总体样本的均值与方差










Decoder部分的masked包含padding mask和look ahead mask（方阵对角），去两者的并集


1.Transformer为何使用多头注意力机制? (为什么不使用一个头)
答：可以从多空间获得信息，类似CNN中的多通道。如果模型只使用单一的注意力头，它可能只会捕捉序列中一些特定类型的模式，但使用多头注意力机制可以更好地平衡模型对不同模式的关注程度。

2.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘?(注意和第一个问题的区别)
答：qkv不同可以保证在不同空间进行投射，增强表达能力，提高泛化能力

3.Transformer计算attention的时候为何选择点乘而不是加法? 两者计算复杂度和效果上有什么区别?
答：为了计算更快，在表达相似度上加法没有点积表现好，因为整个attention计算就是点积运算，实际上，随着维度d的增大，加性模型会明显好于点积模型

4.为什么在进行softmax之前需要对attention进行scaled (为什么除以dk的平方根)，并使用公式推导进行讲解
答：可以从前向传播和反向传播进行解释
前向：假设q和k的各个分量是相互独立的随机变量，它们的均值为0，方差为1，那么qk经过点积之后的均值依然为0，但是方差变为了dk，所以要除以根号dk，把它拉回到μ=0，σ=1的分布中来



5.在计算attention score的时候如何对padding做mask操作?
答：padding_mask，在经过embedding和position后的词向量添加padding项，然后把padding项变为1，其他部分变为0，然后把它乘以一个非常小的数，比如-1e9，通过自动广播之后与qk乘积之后的值相加，此时padding部分就是很小的数，经过softmax时基本为0，即消除了padding无关信息的影响
Look ahead mask，出现在decoder部分，以翻译为例子，要掩盖掉后面的词，以句子长度组成一个方阵，取一个倒三角，然后和padding mask取并集，并集的地方为1，其他为0，剩下的操作和padding mask一致

6.为什么在进行多头注意力的时候需要对每个head进行降维?(可以参考上面一个问题)
答：对每个head进行降维是因为多头注意力中每个head的计算结果都会被用于最终的计算结果，如果head维度过大，那么它在计算中的贡献就会过大，从而产生过拟合影响，所以要降维以抑制它在最终计算中的贡献，另外降维能提高模型的计算效率，如果每个head的维度过大，那么在计算过程中所需要的计算量也相应变大，会影响模型的运行效率。


7.大概讲一下Transformer的Encoder模块?
答：它由多个encoder层组成，每个层包含多个头的多头注意力机制和一个前馈神经网络。
在进行编码时，encoder层会对输入序列进行处理，得到输入序列的内部表示，每个encoder层中的多头注意力机制会根据输入序列的位置信息，计算出不同位置的词的重要性。然后通过前馈神经网络，将这些重要信息与输入序列的词向量相结合，得到输入序列新的表示。
最终，整个模块的encoder层会被拼接，得到最终的输入序列的内部表示。这个内部表示可以用作模型的后续计算，如分类、生成等任务。

8.为何在获取输入词向量之后需要对矩阵乘以embedding size的开方? 意义是什么?
答：因为下一步是对其增加位置编码，我们想扩大embedding的占比，提高它的影响力，所以乘上一个值再与位置编码相加


9.简单介绍一下Transformer的位置编码? 有什么意义和优缺点?
答PE(pos, i) = sin(pos / 10000^(2i / Dmodel)),
PE(pos, i+1) = sin(pos / 10000^(2i / Dmodel)) ,i=Dmodel//2
我们可以看到只要位置和词向量维度确定了,词的绝对位置和相对位置就能确定，因为transformer是并行化操作的，那么就会像cnn那样没有了顺序，此时添加位置编码，就可以知道上下文关系，但是它会在自注意机制经过线性变化后消失
具体实现方法：首先，为每一个词分配一个位置编码向量，该向量的维度和词向量的维度相同。然后将词向量乘以一个大数之后与位置编码向量相加得到词的最终向量表示。
优点：它能有效地考虑词语词与词之间的相对位置关系，提高模型的准确度。
缺点：计算过程中需要额外的计算，会增加模型的计算复杂度。


10.你还了解哪些关于位置编码的技术，各自的优缺点是什么?
答：固定位置编码：这种方法是将每个词的位置信息编码成一个固定的词向量，并与词向量相加得到最终词向量。优点是简单易实现，缺点是无法对位置信息进行训练和微调。

11.简单讲一下Transformer中的残差结构以及意义。
答：把输入和经过变化后的输出一起传递给下一个结构，它的意义很简单，在反向传播求导时，保证已经有了一个固定项，即保证了梯度不会消失

12.为什么transformer块使用LayerNorm而不是BatchNorm? LayerNorm 在Transformer的位置是哪里?
答： LN 对同一样本的不同维度作归一化  BN不同样本的相同维度作归一化，因为这是在进行nlp任务，同一个样本包含的是同一个词向量的信息，对它整体变化并不会改变它的信息，另外batch——size很小的情况下它并不能代表整体，它的位置在encoder，decoder里每一个输出层后面，对输出数据进行归一化操作，提高模型的训练效果。


13.简答讲一下BatchNorm技术，以及它的优缺点。
答：BN优点1.使得损失更加平滑，加快收敛 2.缓解梯度饱和问题（针对sigmoid问题）
BN缺点：batch_size小的时候，用整个batch的均值和方差来模拟全部样本的均值和方差，其并不具有代表性


14.简单描述一下Transformer中的前馈神经网络? 使用了什么激活函数? 相关优缺点?
答：前馈神经网络中使用了多层感知机，主要是计算多头注意力的权重值，以及对输入数据进行非线性变换。
用了gelu函数，它能够relu函数非线性特性的同时，提高神经网络的训练效果

15. Encoder端和Decoder端是如何进行交互的? (在这里可以问一下关于seg2seg的attention知识)
答：通过encoder出来的值，只取k和v，然后和decoder里经过masked attention出来的q相反应，以翻译为例，就是进行两种语言的交互

16. Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别? (为什么需要decoder自注意力需要进行 sequence mask)
答：前面已经提到了look ahead mask，就相当于这里的sequence mask
Encoder的多头注意力机层主要根据词的上下文信息计算权重值，
Decoder的多头注意力机层则会根据词的语义信息和上下文信息来计算权重值

17.Transformer的并行化提现在哪个地方? Decoder端可以做并行化吗?
答：encoder分了多头以后每一个下面的操作都在并行操作，简单讲,就是6个encoder之间是串行,每个encoder中的两个子模块之间是串行,子模块自身是可以并行的。decoder一样，内部子模块可以并行
并行在多头注意力机制中，这种设计的优点是：可以并行计算提升计算效率，此外，由于每个头都是独立的，所以可以根据实际需求调整头的数量，以满足不同的需求。

18.简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗?
答：wordpiece是一种分词技术，它将一个词分为多个子词，以更好地表示词的语义信息。例如将"book" 拆分为 "book" 和 "##ing"，表示这个词是一个动词而不是名词。
byte pair encoding是将重复的字符压缩为较短的符号，来减少字符串的长度，例如，将字符串 "hello world" 压缩为 "h@@ w@@"，表示 "@@" 代表字符 "ello"。

19. Transformer训练的时候学习率是如何设定的? Dropout是如何设定的，位置在哪里? Dropout在测试的需要有什么需要注意的吗?
答：在训练过程中，会根据模型的表现来动态调整学习率，当模型的表现较好时，会增加学习率，以加快模型的收敛速度；当模型表现不佳时，会降低学习率，以避免陷入局部最优解。这种动态调整学习率的方法可以帮助transformer模型更快地收敛，同时避免了固定学习率带来的可能的问题，如收敛速度过慢或陷入局部最优解。
Dropout是一种常见的正则化方法，它在训练时随机让一部分神经元失活，以防止过拟合，在训练transformer时，通常在每一层的输入和输出上使用dropout。例如，在训练自注意力层时，你可能会在每一层的输入和输出上使用dropout，以保护模型的输入和输出不会过拟合。
在测试时，通常不使用dropout，这是因为dropout是用来防止过拟合的，而测试数据不应该包含过拟合模型。因此，在测试时应该要禁用dropout，以便模型能够充分利用它所有的神经元，并且对测试数据进行准确的预测。

20.引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧?
答：BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。

21.谈谈word2vec
答：Word2vec是一种流行的用于词嵌入（word embedding）的方法。它通过分析语料库中的大量文本数据，将每个词语映射到一个低维空间中的向量，使得语料库中的每个词语都有一个对应的词向量。词向量的含义取决于它们在语料库中的上下文，因此相似的词语会被映射到相似的向量上。
Word2vec有两种流行的实现方法：CBOW和Skip-gram。CBOW模型预测每个词语的上下文，而Skip-gram模型则预测每个词语的目标词语。例如，假设我们想要学习单词“cat”的词向量，那么CBOW模型会查看上文和下文中的单词，并尝试预测中间的“cat”这个单词，而Skip-gram模型则会查看“cat”这个单词并尝试预测它的上文和下文中的单词。
总之，Word2vec是一种非常流行的词嵌入技术，它可以将词语映射到一个低维空间中的向量，并能够捕捉词语之间的相似性和上下文关系。

22.谈谈glove
答：Glove（Global Vectors for Word Representation）是一种用于词嵌入（word embedding）的技术，它是基于对语料库中的单词出现的上下文信息进行建模的。它通过分析语料库中的文本数据，将每个词语映射到一个低维空间中的向量，使得语料库中的每个词语都有一个对应的词向量。

Glove模型的目标是通过学习语料库中单词的共现统计信息来捕捉单词之间的相似性。例如，如果“cat”和“dog”经常出现在相似的上下文中，那么Glove模型会将它们映射到相似的向量上，表示它们是相似的词语。

与Word2vec不同，Glove模型并不需要预测单词的上下文或目标单词，而是直接学习单词之间的共现统计信息。这使得Glove模型可以更快地训练，并且可以捕捉单词之间的更复杂的关系。总之，Glove是一种高效的词嵌入技术，可以捕捉单词之间的相似性和上下文关系。

23.谈谈Seq2Seq
答：Seq2Seq（Sequence to Sequence）是一种深度学习模型，它由两个模块组成：一个编码器（encoder）和一个解码器（decoder）。编码器接收一个序列作为输入，并将其编码为一个固定长度的向量，而解码器则接收这个向量作为输入，并解码成另一个序列。

Seq2Seq模型通常用于自然语言处理任务，例如机器翻译、问答系统和对话系统。它的优点在于它可以自动学习序列之间的映射关系，并且可以处理不定长的序列输入和输出。例如，在机器翻译任务中，Seq2Seq模型可以接收一段英文文本作为输入，然后输出相应的法文文本。

总之，Seq2Seq是一种流行的深度学习模型，它可以用于自然语言处理任务，并能够处理不定长的序列输入和输出。

24.不考虑多头的原因，self-attention中词向量不乘QKV参数矩阵，会有什么问题？
答：self_attention的核心是利用文本中的其他词来增强目标词的语义表示，从而更好地利用上下文信息，如果不乘q，模型无法得到语义信息，不乘k，模型无法的词与词之间的关系，不乘v，模型无法得到词的上下文信息，总得来说不乘QKV参数矩阵会导致模型失去理解语言的能力。

25.为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？
答：类似完型填空的做法，也和CBOW有异曲同工之妙。如果mask的词过少，模型就会学习到一些不太有用的信息，如果mask的词过多，模型无法从上下文中学习到足够的信息，15%是一个合理的选择，但是，无论选择什么比例，都要保证模型能够从上下文中学习到足够的信息。

25.使用BERT预训练模型为什么最多只能输入512个词，最多只能两个句子合成一句？
答：是为了显存考虑，即计算资源。

26.为什么BERT在第一句前会加一个[CLS]标志?
答：它不仅用来做输入的第一个词，还用来做模型的输出，[CLS]标志对应的输出向量会被用来表示整个输入的特征。这个输出向量可以被用来进行各种自然语言处理任务，比如分类、问答和句子相似度计算等。

27.Transformer在哪里做了权重共享，为什么可以做权重共享？
答：（1）Encoder和Decoder间的Embedding层权重共享；
（2）Decoder中Embedding层和FC层权重共享。

28.BERT非线性的来源在哪里？
答：self_attention和GeLU是非线性的。

29.为什么BERT比ELMo效果好？
答：（1）基于transformer的特征抽取效果强于ELmo
（2）BERT的训练数据和模型参数均多余ELMo

30.self-attention相比lstm优点是什么？
高效。self-attention通过矩阵乘法来计算注意力，可以通过矩阵分块并行计算，从而提高计算效率。而LSTM需要循环地计算每一个时间步，计算复杂度较高。
易于理解。self-attention的计算过程可以直接通过矩阵运算来表示，直接易于理解。而LSTM的计算过程较为复杂，难以直接理解。
易于训练。self-attention的训练过程可以采用标准的梯度下降法

31.BERT 是如何区分一词多义的？
答： BERT 使用一种称为“上下文无关嵌入”的技术，通过考虑一个单词在句子中的上下文来区分该单词的不同用法。 例如，在句子“我喜欢吃香蕉”中，单词“香蕉”指的是一种水果，而在句子“这是一个香蕉形状的东西”中，单词“香蕉”指的是一个具有香蕉形状的东西。 BERT 通过考虑单词的上下文，可以区分它们不同的意义。

32.BERT的输入是什么，哪些是必须的，为什么position id不用给，type_id 和 attention_mask没有给定的时候，默认会是什么？
答：输入
句子中的单词（按顺序）
每个单词对应的词汇索引
[CLS]和[SEP]标记的词汇索引
这些信息都是 BERT 模型输入的必需内容。 为了使 BERT 更好地模拟人类的语言理解能力，它还会在输入中使用其他信息，例如语言模型和注意力机制。

为什么position id不用给，是因为它采取的是“上下文无关嵌入”，通过考虑一个单词在句子上下文来区分该单词的不同用法。因此，BERT可以在不使用位置ID的情况下区分单词的不同用法。
type_id 是一个整数向量，其中的每个元素都对应于输入句子中的每个单词。 它用于表示单词的类型，例如主语、谓语或宾语。 如果 type_id 没有给定，默认情况下会将所有 type_id 设置为 0，表示所有单词均为相同类型。

attention_mask 是一个二元整数向量，其中的每个元素都对应于输入句子中的每个单词。 它用于表示单词是否应该被 BERT 模型所关注。 如果 attention_mask 没有给定，默认情况下会将所有 attention_mask 设置为 1，表示 BERT 模型应该关注所有单词。

因此，如果 type_id 和 attention_mask 没有给定，默认情况下会将所有 type_id 设置为 0，并将所有 attention_mask 设置为 1。

63.为什么bert需要额外的segment embedding?
答：因为bert预训练的其中一个任务是判断segment A和segment B之间的关系，这就需要embedding中能包含当前token属于哪个segment的信息，然而无论是token embedding，还是position embedding都无法表示出这种信息，因此额外创建一个segment embedding matrix用来表示当前token属于哪个segment的信息，segment vocab size就是2，其中index=0表示token属于segment A，index=1表示token属于segment B。

35.怎么样利用BERT结构针对文本分类任务做fine-tuning？
答：在利用 BERT 模型进行文本分类任务的 fine-tuning 时，通常会进行以下步骤：
(1)使用 BERT 预训练模型对文本进行编码，将每个文本转换为 BERT 模型所能理解的表示。这通常是通过将文本输入 BERT 模型，并获取最后一层输出得到的。
(2)在 BERT 模型的基础上构建一个新的神经网络，用于文本分类。这个神经网络通常包括一个或多个隐藏层，以及一个输出层。输出层的大小应该与分类任务中的类别数量相同。
(3)使用带标签的文本数据对这个新的神经网络进行训练。在训练过程中，可以选择冻结 BERT 模型的参数，以避免将 BERT 模型的预训练结果改变。
在训练完成后，将整个模型部署到生产环境中，用于处理实际的文本分类任务。

366.bert中mask方式与cbow有什么异同点？
答：相同点：CBOW的核心思想是：给定上下文，根据它的上文 Context-Before 和下文 Context-after 去预测input word。而BERT本质上也是这么做的，但是BERT的做法是给定一个句子，会随机Mask 15%的词，然后让BERT来预测这些Mask的词。
不同点：首先，在CBOW中，每个单词都会成为input word，而BERT不是这么做的，原因是这样做的话，训练数据就太大了，而且训练时间也会非常长。
其次，对于输入数据部分，CBOW中的输入数据只有待预测单词的上下文，而BERT的输入是带有[MASK] token的“完整”句子，也就是说BERT在输入端将待预测的input word用[MASK] token代替了。
另外，通过CBOW模型训练后，每个单词的word embedding是唯一的，因此并不能很好的处理一词多义的问题，而BERT模型得到的word embedding(token embedding)融合了上下文的信息，就算是同一个单词，在不同的上下文环境下，得到的word embedding是不一样的。


33.BERT训练时使用的学习率 warm-up 策略是怎样的？为什么要这么做？
答：在训练 BERT 模型时，通常会使用“warm-up”学习率策略。 这种策略包括在模型训练的开始阶段使用较小的学习率，并在随后的训练过程中逐渐增加学习率。

使用 warm-up 学习率策略的原因是，当训练模型时，模型的权重可能会在训练的开始阶段变化得非常大。 如果在这个阶段使用较大的学习率，模型的权重可能会变化得过于剧烈，从而导致模型的性能变差。 使用 warm-up 学习率策略可以避免这种情况，因为它能够缓慢地增加学习率，从而让模型的权重变化得更加平缓。
另外，使用 warm-up 学习率策略还能够有助于模型收敛到一个更优的解。 因为模型的权重会在训练的开始阶段变化得比较大，所以如果不使用 warm-up 学习率策略，模型可能会收敛到一个不太优的解。 使用 warm-up 学习率策略可以更好地控制模型的权重变化，从而有助于模型收敛到一个更优的解。

34.为什么说ELMO是伪双向，BERT是真双向？产生这种差异的原因是什么？
答：ELMO 模型只能够从左向右读取句子，并在学习语言表示时只考虑单词的前向上下文。 例如，在句子“我喜欢吃香蕉”中，ELMO 模型可以通过考虑单词“喜欢”左侧的单词“我”来学习“喜欢”的语言表示。 因为 ELMO 模型只能从左向右读取句子，所以它不能够考虑单词“喜欢”右侧的单词“吃”，因此它的表示能力受到了限制。
BERT 模型则能够从左向右和从右向左读取句子，并在学习语言表示时考虑单词的前向和后向上下文。 例如，在句子“我喜欢吃香蕉”中，BERT 模型可以通过考虑单词“喜欢”左侧的单词“我”和右侧的单词“吃”来学习“喜欢”的语言表示。 因为 BERT 模型能够同时考虑单词的前向和后向上下文，所以它的表示能力比 ELMO 模型更强。
因此，ELMO 模型被称为“伪双向”。

35.BERT和Transformer Encoder的差异有哪些？做出这些差异化的目的是什么？
答： BERT 模型使用了多层的 Transformer Encoder，并且这些层之间存在相互连接。 这样的网络结构使 BERT 模型能够在学习语言表示时考虑单词的前向和后向上下文。
Transformer Encoder 模型的单层网络结构限制了它在学习语言表示时可以考虑的信息。 它只能从左向右读取句子，并在学习语言表示时只考虑单词的前向上下文。 这使得它的表示能力受到了限制，因此不能够像 BERT 模型那样捕捉到语言中的潜在表示。

36.BERT训练过程中的损失函数是什么？
答：其中一种常用的损失函数是 Masked Language Modeling (MLM) 损失函数。 在 MLM 任务中，BERT 模型需要预测一些被掩盖（masked）的单词。 为了优化模型的表示能力，BERT 模型会根据预测的结果计算一个损失值，并通过反向传播来更新模型的参数。

另外一种常用的损失函数是 Next Sentence Prediction (NSP) 损失函数。 在 NSP 任务中，BERT 模型需要判断一个句子是否与另一个句子相关。 为了优化模型的表示能力，BERT 模型会根据预测的结果计算一个损失值，并通过反向传播来更新模型的参数。

BERT 模型还会使用其他的损失函数来优化模型的表示能力。 例如，它可以使用一些监督学习任务的损失函数来训练模型。 总的来说，BERT 模型会使用多种损失函数来优化模型的表示能力，从而达到最优的训练效果。

55.BERT 的两个任务 Masked LM 任务和 Next Sentence Prediction 任务是先后训练的还是交替训练的
答：同时

56.Attention与全连接层的区别何在？
答：全连接层（fully-connected layer）是指神经网络中的一层，它的每个节点都与前一层的所有节点相连。 全连接层的作用是将前一层的输出映射到下一层的输入，并且可以通过学习权重参数来实现复杂的非线性变换。

Attention 层是指在深度学习模型中使用的一种特殊的层，它可以帮助模型更好地利用序列数据中的上下文信息。 Attention 层通过计算序列中每个元素与上下文信息之间的相似度来得到一个权重值，然后根据权重值来计算输出。 这样的处理方式可以帮助模型更好地理解序列数据中的上下文信息，并且可以提高模型的泛化能力。

因此，Attention 层和全连接层有着不同的作用。 Attention 层用于处理序列数据，可以帮助模型更好地理解上下文信息；而全连接层则用于将前一层的输出映射到下一层的输入，并且可以实现复杂的非线性变换。 两者都是深度学习模型中常用的层，但是在不同的应用场景下有着不同的作用。

2222.self-attention 的本质是什么？
答：通常情况下，注意力机制是指在处理序列数据时，模型通过计算序列中每个元素与上下文信息之间的相似度来得到一个权重值，然后根据权重值来计算输出。 这样的处理方式可以帮助模型更好地理解序列数据中的上下文信息，并且可以提高模型的泛化能力。
self-attention 则是在这种基础上的一种变体，它的本质是让模型的输入序列中的每个元素都可以作为上下文信息来计算相似度。 这种方法可以更好地捕捉序列中元素之间的相互依赖关系，并且可以提高模型的表示能力。


37.解释反向传播
答：反向传播（backpropagation）是一种用于训练神经网络的方法。 它通过计算损失函数对模型参数的梯度来更新模型的参数。

在训练神经网络时，反向传播算法会先根据输入数据计算出预测结果，然后根据预测结果和标签计算出损失函数的值。 接着，反向传播算法会计算损失函数对模型参数的梯度，并使用梯度下降算法来更新模型的参数。

通过这种方式，反向传播算法能够不断地更新模型的参数，从而使模型的预测结果更加精确。

38.NLP 项目中哪些是文本预处理的重要步骤？
答：在 NLP 项目中，文本预处理是一个非常重要的步骤。 文本预处理主要包括以下几个重要步骤：

1.去除噪声数据：在文本数据中，会存在一些干扰信息，例如网页标签、表格数据等。 需要将这些干扰信息去除，以保证文本数据的质量。

2.分词：分词是文本预处理中的一个重要步骤。 通过分词，能够将文本数据划分为单独的词语，从而方便进行模型的训练和应用。

3.词干提取：词干提取是一种用于处理词形变化的方法。 它能够将单词的不同形态转换为同一个词干，从而使得模型在处理文本数据时更加高效。

4.去除停用词：停用词是指在文本数据中出现频率很高，但对文本意义分析没有太大贡献的词语。 通过去除停用词，能够减少模型在处理文本数据时的计算量，并使模型更加注重有意义的词语。

5.词性标注：词性标注是一种用于标记每个词语的词性的方法。 通过词性标注，能够帮助模型更准确地理解文本的语义，并使模型的应用更加精准。

6.词向量化：词向量化是一种用于表示词语的方法。 它将每个词语映射为一个数值向量，从而使模型能够处理文本数据。 词向量化能够帮助模型更好地学习文本中的语义信息，并使模型的应用更加精准。
上述步骤是文本预处理的重要步骤，它们能够帮助模型更好地处理文本数据，并使模型的应用更加精准。

39.哪项技巧可用于关键词归一化
答：词干提取是一种用于处理词形变化的方法。 它能够将单词的不同形态转换为同一个词干，从而使得模型在处理文本数据时更加高效。 例如，通过词干提取，可以将单词 "bought" 和 "buy" 都转换为 "buy"，从而使得模型能够统一处理这两个单词。

另外一种技巧是词性标注。 词性标注是一种用于标记每个词语的词性的方法。 通过词性标注，能够帮助模型更准确地理解文本的语义，并使模型的应用更加精准。 例如，通过词性标注，可以区分单词 "book" 在不同上下文中的词性，从而使得模型能够更准确地理解文本。

通过使用词干提取和词性标注等技巧，可以更有效地处理关键词归一化问题，从而提升模型在处理文本数据时的准确性。

此外，还可以使用词向量化技巧来解决关键词归一化问题。 词向量化是一种用于表示词语的方法。 它将每个词语映射为一个数值向量，从而使模型能够处理文本数据。 词向量化能够帮助模型更好地学习文本中的语义信息，并使模型的应用更加精准。 例如，通过词向量化，可以将单词 "book" 和 "books" 映射到相似的词向量，从而使得模型能够更准确地理解这两个单词的含义。

总的来说，词干提取、词性标注和词向量化等技巧可以有效地处理关键词归一化问题，从而提升模型在处理文本数据时的准确性。 使用这些技巧，模型能够更准确地理解文本数据中的语义信息，并使模型的应用更加精准。

2656.谈谈深度学习中的Dense操作
答：Dense 操作是一种常用的神经网络操作。它将输入层的所有节点与后面的层中的所有节点相连，这意味着每个输入节点都会与后面层中的所有节点相连。这与常见的全连接层（fully connected layer）是相同的。
在深度学习模型中，Dense 操作通常用来实现全连接层。这些层通常位于模型的最后一个或倒数第二个，用于将模型抽象的特征表示转换为最终的预测结果。例如，如果模型是用来分类图像，那么最后一层可能会将模型抽象的特征表示映射到每个类别的概率分布。
Dense 操作是一种非常常见的神经网络操作，用于实现全连接层。它可以帮助深度学习模型捕捉输入数据中的复杂模式，并将这些模式映射到最终的预测结果。

5556.动量随机梯度下降
在每次迭代中，动量随机梯度下降算法会计算当前梯度值，并根据动量项来更新参数值。动量项是一个累积的运动量，它与当前的梯度值和上一次迭代的参数值有关。如果当前的梯度值与上一次迭代的参数值方向相同，那么动量项就会增大，从而提高下一次迭代的收敛速度。如果当前的梯度值与上一次迭代的参数值方向相反，那么动量项就会减小，从而减缓下一次迭代的收敛速度。
通过引入动量项，动量随机梯度下降算法能够在每次迭代中更快地收敛，并且能够更好地应对梯度值的震荡。这样，算法就能够在更短的时间内达到更优的结果。



12A.独立同分布
独立同分布（Independent and Identically Distributed，简称 IID）是指一组随机变量（或数据样本）的概率分布都相同且互相独立。这种特殊的概率分布结构在概率论和统计学中都有着重要的意义。
对于一组独立同分布的随机变量，它们的联合概率分布就是由它们的各自概率分布的乘积得到的。独立同分布的随机变量之间不存在任何相关性，因此可以通过单独考虑每个随机变量来分析它们的性质。
在机器学习领域，独立同分布的数据样本是用来进行统计推断和模型训练的常见条件之一。例如，在模型训练过程中，假设数据样本是独立同分布的，可以推导出用极大似然估计来估计模型参数的最优解。




过拟合欠拟合（举几个例子让判断下，顺便问问、超参数搜索方法、EarlyStopping）
交叉验证的目的
答：评估模型的泛化能力，

L1正则和L2正则的做法、正则化背后的思想（顺便问问BatchNorm、Covariance Shift）、L1正则产生稀疏解原理、逻辑回归为何线性模型（顺便问问LR如何解决低维不可分、从图模型角度看LR和朴素贝叶斯和无监督）、几种参数估计方法MLE/MAP/贝叶斯的联系和区别、简单说下SVM的支持向量（顺便问问KKT条件、为何对偶、核的通俗理解）、 GBDT随机森林能否并行（顺便问问bagging boosting）、 生成模型判别模型举个例子、聚类方法的掌握（顺便问问Kmeans的EM推导思路、谱聚类和Graph-cut的理解）、梯度下降类方法和牛顿类方法的区别（顺便问问Adam、L-BFGS的思路）、半监督的思想（顺便问问一些特定半监督算法是如何利用无标签数据的、从MAP角度看半监督）、常见的分类模型的评价指标（顺便问问交叉熵、ROC如何绘制、AUC的物理含义、类别不均衡样本）

CNN中卷积操作和卷积核作用、maxpooling作用、卷积层与全连接层的联系、梯度爆炸和消失的概念（顺便问问神经网络权值初始化的方法、为何能减缓梯度爆炸消失、CNN中有哪些解决办法、LSTM如何解决的、如何梯度裁剪、dropout如何用在RNN系列网络中、dropout防止过拟合）、为何卷积可以用在图像/语音/语句上（顺便问问channel在不同类型数据源中的含义）

如果面试者跟我一样做NLP、推荐系统，我会继续追问 CRF跟逻辑回归 最大熵模型的关系、CRF的优化方法、CRF和MRF的联系、HMM和CRF的关系（顺便问问 朴素贝叶斯和HMM的联系、LSTM+CRF 用于序列标注的原理、CRF的点函数和边函数、CRF的经验分布）、WordEmbedding的几种常用方法和原理（顺便问问language model、perplexity评价指标、word2vec跟Glove的异同）、topic model说一说、为何CNN能用在文本分类、syntactic和semantic问题举例、常见Sentence embedding方法、注意力机制（顺便问问注意力机制的几种不同情形、为何引入、seq2seq原理）、序列标注的评价指标、语义消歧的做法、常见的跟word有关的特征、factorization machine、常见矩阵分解模型、如何把分类模型用于商品推荐（包括数据集划分、模型验证等）、序列学习、wide&deep model（顺便问问为何wide和deep)

关于逻辑回归
什么是逻辑回归（Logistic Regression）？
是一种常用的分类方法，用于处理因变量为二元或多元离散变量的情况。
逻辑回归和线性回归有什么区别？
逻辑回归和线性回归的主要区别在于它们处理的因变量类型不同。线性回归用于处理连续型因变量，而逻辑回归用于处理离散型因变量。
逻辑回归的目标函数是什么？
逻辑回归的目标函数通常为对数似然函数（Log-likelihood function），它衡量了模型预测结果与真实结果之间的拟合程度。
【就是交叉熵损失函数】J(θ) = -Σ[yi * log(h(xi)) + (1 - yi) * log(1 - h(xi))]
如何用梯度下降法（Gradient Descent）求解逻辑回归问题？
梯度下降法（Gradient Descent）是一种常用的优化算法，可以用来求解逻辑回归问题。它通过迭代更新模型参数来最小化目标函数。
什么是正则化（Regularization），它在逻辑回归中有什么作用？
正则化（Regularization）是一种防止过拟合（Overfitting）的方法，它通过在目标函数中添加一个正则项来限制模型复杂度。
如何评估逻辑回归模型的性能？
可以使用准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1分数（F1-score）等指标来评估逻辑回归模型的性能。
如何处理多分类问题（Multiclass Classification）？
多分类问题（Multiclass Classification）指的是因变量有多于两个类别的分类问题。可以使用一对多（One-vs-Rest）或一对一（One-vs-One）等方法来解决多分类问题。
什么是类别不平衡问题（Class Imbalance），如何解决这个问题？
类别不平衡问题（Class Imbalance）指的是数据集中不同类别的样本数量不均衡的情况。可以使用过采样（Oversampling）、欠采样（Undersampling）或合成少数类过采样技术（SMOTE）等方法来解决类别不平衡问题。
如何选择逻辑回归模型的超参数（Hyperparameters）？
超参数（Hyperparameters）是指在模型训练之前需要设定的参数，例如正则化系数、学习率等。可以使用网格搜索（Grid Search）、随机搜索（Random Search）或贝叶斯优化（Bayesian Optimization）等方法来选择超参数。
逻辑回归有哪些局限性，如何克服这些局限性？
逻辑回归的局限性包括它只能处理线性可分数据、对异常值敏感、无法处理高维数据等。可以使用核技巧（Kernel trick）、异常值检测和删除、特征选择和降维等方法来克服这些局限性。







BPE算法：
1.获取语料→拆分→加后缀→统计词频
以此句为例“FloydHub is the fastest way to build, train and deploy deep learning models. Build deep learning models in the cloud. Train deep learning models.”


2.建立词表→统计字符频率→顺便排序

3.以第一次迭代为例，将字符频率最高的 d 和 e 替换为 de，后面依次迭代


4.更新词表











